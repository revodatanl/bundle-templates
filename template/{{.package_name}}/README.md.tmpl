# {{.package_name}}

[![python](https://upload.wikimedia.org/wikipedia/commons/1/16/Blue_Python_3.10%2B_Shield_Badge.svg)](https://www.python.org)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)

[![unit_tests](https://github.com/revodatanl/{{.package_name}}/actions/workflows/unit_test.yml/badge.svg)](https://github.com/revodatanl/{{.package_name}}/actions/workflows/unit_test.yml)
[![check_formatting](https://github.com/revodatanl/{{.package_name}}/actions/workflows/check_formatting.yml/badge.svg)](https://github.com/revodatanl/{{.package_name}}/actions/workflows/check_formatting.yml)
[![check_python_packaging](https://github.com/revodatanl/{{.package_name}}/actions/workflows/check_python_packaging.yml/badge.svg)](https://github.com/revodatanl/{{.package_name}}/actions/workflows/check_python_packaging.yml)
[![deploy](https://github.com/revodatanl/{{.package_name}}/actions/workflows/deploy.yml/badge.svg)](https://github.com/revodatanl/{{.package_name}}/actions/workflows/deploy.yml)

The '{{.package_name}}' project was generated by using the RevoData template.

## Getting started

1. Install the Databricks CLI from <https://docs.databricks.com/dev-tools/cli/databricks-cli.html>

2. Authenticate to your Databricks workspace, if you have not done so already:

   ```bash
   databricks configure
   ```

3. To deploy a development copy of this project, type:

   ```bash
   databricks bundle deploy --target dev
   ```

   (Note that "dev" is the default target, so the `--target` parameter
   is optional here.)

   This deploys everything that's defined for this project.
   For example, the default template would deploy a job called
   `[dev yourname] {{.package_name}}_job` to your workspace.
   You can find that job by opening your workpace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:

   ```bash
   databricks bundle deploy --target prod
   ```

   Note that the default job from the template has a schedule that runs every day
   (defined in resources/{{.package_name}}_job.yml). The schedule
   is paused when deploying in development mode (see
   <https://docs.databricks.com/dev-tools/bundles/deployment-modes.html>).

5. To run a job or pipeline, use the "run" command:

   ```bash
   databricks bundle run
   ```

6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   <https://docs.databricks.com/dev-tools/vscode-ext.html>. Or read the "getting started" documentation for
   **Databricks Connect** for instructions on running the included Python code from a different IDE.

7. For documentation on the Databricks asset bundles format used
   for this project, and for CI/CD configuration, see
   <https://docs.databricks.com/dev-tools/bundles/index.html>.
