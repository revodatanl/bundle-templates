{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55df3c5-ef38-473a-b5d1-c1362e4ecce6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Staging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e1d9c93-8a4b-4411-b278-f5d9165e70f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load libraries and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Write a function to extract the columns type from a metadata table\n",
    "from {{.package_name}}.table_utils import extract_columns_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not on Databricks, start a local spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"spark\" not in locals():\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"local_spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = spark.conf.get(\"table_name\")\n",
    "metadata_path = spark.conf.get(\"metadata_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = spark.read.csv(path=f\"{metadata_path}/{table_name}.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42463664-fa7c-430d-829e-27a7570681c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform Prestaging to Staging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4615ff6-908f-49c5-836d-d36235fdae87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\n",
    "    name=f\"staging_{table_name}\",\n",
    "    comment=\"SCD2 implemented\",\n",
    "    table_properties={\"quality\": \"silver\"},\n",
    ")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target=f\"staging_{table_name}\",\n",
    "    source=f\"prestaging_{table_name}\",\n",
    "    keys=extract_columns_type(df_metadata, \"Key\"),\n",
    "    sequence_by=F.col(\"actuality_date\"),\n",
    "    except_column_list=[\"actuality_date\", \"input_file_name\"],\n",
    "    ignore_null_updates=False,\n",
    "    track_history_column_list=extract_columns_type(df_metadata, \"Attribute\"),\n",
    "    stored_as_scd_type=\"2\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pushcart_staging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
