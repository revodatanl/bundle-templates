defaults: &default_task_config
  run_if: ALL_SUCCESS
  libraries:
    - whl: ../../dist/*.whl
  timeout_seconds: 0

resources:
  jobs:
    example_dataset:
      name: example_dataset

      tasks:
        - task_key: dlt_ingestion
          pipeline_task:
            pipeline_id: ${resources.pipelines.dlt_example_dataset.id}

      max_concurrent_runs: 1

      parameters:
        - name: table_name
          default: example_dataset

      email_notifications:
        on_failure:
          - {{.email}}

      schedule:
        # Run every day at 8:37 AM
        quartz_cron_expression: "44 37 8 * * ?"
        timezone_id: Europe/Amsterdam

      job_clusters:
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: "${var.spark_version}"
            instance_pool_id: ${var.default_pool}
            driver_instance_pool_id: ${var.default_pool}
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 1
            spark_conf:
              spark.databricks.delta.optimizeWrite.enabled: "true"

  pipelines:
  dlt_example_dataset:
    name: dlt_example_dataset
    target: "dlt"
    libraries:
      - notebook:
          path: ../notebooks/dlt_ingestion.ipynb

    clusters:
      - label: default
        num_workers: 1
        node_type_id: Standard_D4s_v3

    configuration:
      bundle.sourcePath: /Workspace/${workspace.file_path}
      table_name: example_dataset
      autoloader_path: autoloader/path
      metdata_path: metadata/path
      bundle.env: ${bundle.environment}
      pipelines.allowCustomSchemaForSql: "true"
      pipelines.allowCustomSchemaForPython: "true"

    continuous: false
    photon: false
    development: true

    channel: PREVIEW
